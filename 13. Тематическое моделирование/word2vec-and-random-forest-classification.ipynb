{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb32d1a1-a976-da68-5b42-9d095ed3a2f1"
   },
   "source": [
    "# word2vec and random forest classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4647ae4d-a164-f996-8ba6-201798aa05f2"
   },
   "source": [
    "Analysis of the [Amazon Fine Food Reviews dataset on Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews).\n",
    "GitHub repository with this analysis: https://github.com/JungeAlexander/kaggle-amazon-fine-food-reviews\n",
    "\n",
    "**Summary:** The goal of the analysis is to classify positive (4-5 stars) and negative (1-2 stars) reviews based on the\n",
    "review content. Prior to classification using a random forest, vector representations (aka word embeddings)\n",
    "are learned using [word2vec](https://www.tensorflow.org/versions/r0.12/tutorials/word2vec/index.html). After exploring these word embeddings a little bit, the embeddings are used to map each review to a feature vector.\n",
    "These feature vectors are then used in the classfication task.\n",
    "\n",
    "---\n",
    "\n",
    "Some inspiration for this notebook can be found here:\n",
    "\n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
    "- https://www.kaggle.com/gpayen/d/snap/amazon-fine-food-reviews/building-a-prediction-model/notebook\n",
    "- https://www.kaggle.com/inspector/d/snap/amazon-fine-food-reviews/word2vec-logistic-regression-0-88-auc/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6edd3adf-931f-a21f-1123-a0319faa5fee"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, word2vec\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sqlite3\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "6df5d31f-d1ee-648d-d4ea-28e6be7706a4"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5c401098-0f0b-baa8-b523-2e85104e9a56"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/20223210/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer used for splitting reviews into sentences\n",
    "nltk.download('punkt')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "361dbfe2-6700-116a-5e72-52fa75fbac83"
   },
   "source": [
    "# Labelling good and bad reviews\n",
    "\n",
    "To obtain binary class labels, reviews with 4-5 stars are considered good and assigned a '1' class label while reviews with less than 3 stars are considered bad and assigned a '0' class label. Reviews with 3 stars are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "d87fd413-c646-5d46-cad5-9b93fc86dbb7"
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w7/r_mz1frd4hbdbnlm1hh0c7xxhz1_lw/T/ipykernel_35494/1439562094.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/database.sqlite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "connection = sqlite3.connect('../input/database.sqlite')\n",
    "reviews = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\", connection)\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ddc43bae-e1a5-6884-2813-a1b596c600bc"
   },
   "outputs": [],
   "source": [
    "reviews.hist('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "202d99f1-24ed-8c85-4e86-f25fcda61af2"
   },
   "source": [
    "We observe that the data set is heavily skewed towards reviews with a score of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8d2773d3-69fb-8796-b7ab-3ed2cbda7a2e"
   },
   "outputs": [],
   "source": [
    "reviews['Class'] = 1 * (reviews['Score'] > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ba0c2d0-6636-5091-d2a8-446d5ee3793f"
   },
   "outputs": [],
   "source": [
    "reviews.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "87ac6f7f-1c25-af79-f29d-46b16b41d621"
   },
   "source": [
    "## Split into training and test sets\n",
    "\n",
    "Split the data set into training and test sets. To ensure indendence of training and test set and generalizability of the model, make sure that no product (identified by `ProductId`) and no user (identified by `UserId`) is present in both training and test set.\n",
    "\n",
    "This is implemented by first sorting the data set by `ProductId`, splitting into equally sized training and test set (no shuffling!) and lastly removing any reviews from the test set where either user or product ID also appears in the training set. Of course, this will make the training and test sets unequally sized but ¯\\\\\\_\\_(ツ)\\_\\_/¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5eadc421-f29e-6206-da09-c5d721d68745",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews.sort_values('ProductId', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "40ec45dd-d3c7-6ead-ae69-bfd5b1c2fca7"
   },
   "outputs": [],
   "source": [
    "train_size = int(len(reviews) * 0.5)\n",
    "train_reviews = reviews.iloc[:train_size,:]\n",
    "test_reviews = reviews.iloc[train_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6d57cee5-e086-0077-0fde-321411a336e7"
   },
   "outputs": [],
   "source": [
    "test_remove = np.logical_or(test_reviews['ProductId'].isin(train_reviews['ProductId']),\n",
    "                          test_reviews['UserId'].isin(train_reviews['UserId']))\n",
    "test_reviews = test_reviews[np.logical_not(test_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c2ad92d-242a-e6e7-0e0a-49b12a109885"
   },
   "outputs": [],
   "source": [
    "print('Training set contains {:d} reviews.'.format(len(train_reviews)))\n",
    "print('Test set contains {:d} reviews ({:d} removed).'.format(len(test_reviews), sum(test_remove)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3fbf9db9-7887-7915-e04f-a57fb5a11fe9"
   },
   "outputs": [],
   "source": [
    "n_pos_train = sum(train_reviews['Class'] == 1)\n",
    "print('Training set contains {:.2%} positive reviews'.format(n_pos_train/len(train_reviews)))\n",
    "n_pos_test = sum(test_reviews['Class'] == 1)\n",
    "print('Test set contains {:.2%} positive reviews'.format(n_pos_test/len(test_reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32579c79-06a2-1a90-f189-d6d692c93e0d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del reviews  # hint for garbage collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b2c4b38b-4aef-3f65-06db-9aff3f9ed72d"
   },
   "source": [
    "## Preparing review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "44e6b13b-ac5a-3be7-90c6-6ffdaac51434"
   },
   "source": [
    "Convert each review in the training set to a list of sentences where each sentence is in turn a list of words.\n",
    "Besides splitting reviews into sentences, non-letters and (optionally) stop words are removed and all words\n",
    "coverted to lower case.\n",
    "\n",
    "Inspired by: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc29a48c-2686-4d6f-d902-ba322261e25c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Convert a review to a list of words. Removal of stop words is optional.\n",
    "    \"\"\"\n",
    "    # remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "    \n",
    "    # convert to lower case and split at whitespace\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    # remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f54f735-3892-2a14-dd37-e5a5cde7370d"
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Split review into list of sentences where each sentence is a list of words.\n",
    "    Removal of stop words is optional.\n",
    "    \"\"\"\n",
    "    # use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    # each sentence is furthermore split into words\n",
    "    sentences = []    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38ac484d-0b44-0d5d-ac32-55a6b5eba5cc"
   },
   "outputs": [],
   "source": [
    "train_sentences = []  # Initialize an empty list of sentences\n",
    "for review in train_reviews['Text']:\n",
    "    train_sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "02423be9-ef1c-d86d-2327-775a4782e4af"
   },
   "outputs": [],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "639a9f69-af24-4fa8-666f-cfd2d01132f6"
   },
   "source": [
    "## Training a word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c92e6dde-9d9d-3bf2-5b11-61b74c77d9fa"
   },
   "outputs": [],
   "source": [
    "model_name = 'train_model'\n",
    "# Set values for various word2vec parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 3       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "if not os.path.exists(model_name): \n",
    "    # Initialize and train the model (this will take some time)\n",
    "    model = word2vec.Word2Vec(train_sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e09714e9-1b6e-e7b1-bd2b-18b759fbd71a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d36eae9f-5e48-954a-7db3-6062a9f44683"
   },
   "source": [
    "The vector representations for words allow us to explore what the model learned. Using the distance between embedded words, we can find that are similar or dissimilar to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae5b892e-dd0a-d703-93f6-82d37e00b898"
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"banana apple orange sausage\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b43b7cc0-bbad-7c86-82ff-4f8c22cbed18"
   },
   "outputs": [],
   "source": [
    "model.doesnt_match(\"vanilla chocolate cinnamon dish\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0a43ebd-4ad0-61cc-ecaa-b56559f3312a"
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8bb7df2b-b261-64f6-de6d-a38343881258"
   },
   "outputs": [],
   "source": [
    " model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "172ff71b-9489-8355-a9a0-13467405ea12"
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a7c99ec6-f43e-e5a2-dc66-4265fc7e8c13"
   },
   "outputs": [],
   "source": [
    "model.similar_by_vector(model['beer'] - model['alcohol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b77881e9-f673-1732-e027-031fc064dbbe"
   },
   "source": [
    "## Build classifier using word embedding\n",
    "\n",
    "Inspiration: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors\n",
    "\n",
    "Each review is mapped to a feature vector by averaging the word embeddings of all words in the review. These features are then fed into a random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc355ed6-dc19-17c4-2682-27d93a82c23b"
   },
   "outputs": [],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dca3d34d-47c3-a1f3-e16d-abd960b2beed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.index2word)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec,model[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec\n",
    "\n",
    "\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors for all reviews\n",
    "    \"\"\"\n",
    "    counter = 0.\n",
    "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return review_feature_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1f4ef56c-e813-705d-cf54-2cb3a614b320"
   },
   "outputs": [],
   "source": [
    "# calculate average feature vectors for training and test sets\n",
    "clean_train_reviews = []\n",
    "for review in train_reviews['Text']:\n",
    "    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "trainDataVecs = get_avg_feature_vecs(clean_train_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d47d12cd-1b8d-238c-25a9-0540956679ee"
   },
   "outputs": [],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test_reviews['Text']:\n",
    "    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\n",
    "testDataVecs = get_avg_feature_vecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d27d8907-d195-5012-e116-35fc764c3e49"
   },
   "outputs": [],
   "source": [
    "# Fit a random forest to the training data, using 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "\n",
    "print(\"Fitting a random forest to labeled training data...\")\n",
    "forest = forest.fit(trainDataVecs, train_reviews['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc6f3bfb-5e06-349b-c2a3-d4504fcc6eef"
   },
   "outputs": [],
   "source": [
    "# remove instances in test set that could not be represented as feature vectors\n",
    "nan_indices = list({x for x,y in np.argwhere(np.isnan(testDataVecs))})\n",
    "if len(nan_indices) > 0:\n",
    "    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n",
    "    testDataVecs = np.delete(testDataVecs, nan_indices, axis=0)\n",
    "    test_reviews.drop(test_reviews.iloc[nan_indices, :].index, axis=0, inplace=True)\n",
    "    assert testDataVecs.shape[0] == len(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2e71b5cc-59b9-6fd1-2c7a-e07436af3008"
   },
   "outputs": [],
   "source": [
    "print(\"Predicting labels for test data..\")\n",
    "result = forest.predict(testDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4edeaaaa-9640-14fa-269c-3f6fc4f94be7"
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_reviews['Class'], result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1441e318-a82f-ff34-1d7d-86be49956418"
   },
   "source": [
    "While precision looks okay for both negative and positive reviews, recall is only acceptable for positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "196d2ca6-3a26-4d48-4eae-d56c1017f2f6"
   },
   "source": [
    "Finally, compute ROC curve and area under the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7854e6ec-bc66-387f-a9f1-89d1d44edc06",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = forest.predict_proba(testDataVecs)[:, 1]\n",
    "\n",
    "fpr, tpr, _ = roc_curve(test_reviews['Class'], probs)\n",
    "auc = roc_auc_score(test_reviews['Class'], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1638578e-f473-fc94-1036-b2c61aae52ae"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='AUC {:.3f}'.format(auc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "50f5d3ff-00b7-5375-f4dc-e6a75554bd78"
   },
   "source": [
    "## Some ideas for further analyses\n",
    "\n",
    "- Does using a class-balanced training set increase the classification performance?\n",
    "- Parameter tuning for word2vec and random forest (using K-fold cross-validation with small K as training lots of embeddings and classifiers is expensive?)\n",
    "- Remove reviews where too few word embeddings were averaged (see the example that had to be removed)?\n",
    "- How would a classifier with tf-idf features (and no word2vec embedding) perform?\n",
    "- Other model: naive bayes (`MultinomialNB`)? SVM (`SGDClassifier`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8674ef82-b122-4244-3388-db2fd3e0063d"
   },
   "source": [
    "## Random forest classifier without word2vec\n",
    "\n",
    "Instead of word2vec-derived features, the classifier below uses as [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)-derived feature matrix as input to a random forest classifier.\n",
    "We see that F1-score is worse for both classes, compared to the word2vec approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9e02af0e-d392-c900-44e0-440ba2111e0c"
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', MultinomialNB)])\n",
    "\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "#                      ('tfidf', TfidfTransformer()),\n",
    "#                      ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])\n",
    "                     \n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier(n_estimators = 100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "826ebf37-8ebb-9e4a-2ba4-c4b8ac7a7eb2"
   },
   "outputs": [],
   "source": [
    "_ = text_clf.fit(train_reviews['Text'], train_reviews['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f60f8eb6-1cbc-c13a-534a-fb2d999e9c5c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(test_reviews['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "87168fc9-89b5-8c51-5539-a0d0f44415e0"
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_reviews['Class'], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c30e4a5-4f5f-c9f1-eb70-329a29d53c6d"
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_reviews['Class'], predicted))"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 1,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
