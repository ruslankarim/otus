{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb9f484",
   "metadata": {},
   "source": [
    "# Введение в PyTorch\n",
    "\n",
    "<img src=\"./images/pytorch_logo.png\" width=240>\n",
    "\n",
    "**На этом уроке мы рассмотрим как при помощи PyTorch:**\n",
    "- Работать с данными \n",
    "- Создавать свои модели нейросетей (на примере полносвязных и сверточных сетей)\n",
    "- Обучать созданные модели\n",
    "- Сохранять и загружать обученные модели (checkpoints)\n",
    "- Делать предсказания (инференс)\n",
    "- Ускорить инференс при помощи компиляции моделей (PyTorch 2.0)\n",
    "- Transfer Lerning из одного датасета в другой\n",
    "\n",
    "В качестве датасетов будем использовать MNIST и CIFAR10. \n",
    "\n",
    "**Для начала импортируем необходимые библиотеки:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a7faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab4864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательная функция для визуализации данных:\n",
    "def show(img, title=''):\n",
    "    plt.title(title)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9278170",
   "metadata": {},
   "source": [
    "## 1 Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb703475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f5b57",
   "metadata": {},
   "source": [
    "**Загрузка датасета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43bd83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачиваем данные для обучения:\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"./dataset\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Скачиваем данные для теста:\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"./dataset\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008af4c",
   "metadata": {},
   "source": [
    "**Визуализация данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6937d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Опишем название классов:\n",
    "labels_map = ['0','1','2','3','4','5','6','7','8','9']\n",
    " \n",
    "# Визуализируем рандомные сэмплы из MNIST:\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\") # squeeze() - удаляем все измерения размером 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e149f339",
   "metadata": {},
   "source": [
    "**Итерирование через данные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df199720",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Создаем data loaders:\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c50bc2",
   "metadata": {},
   "source": [
    "## 2 Создание модели\n",
    "<img src=\"images/nn_picture.png\">\n",
    "\n",
    "#### Слои\n",
    "[**Flatten**](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) -  трансформирует 2D изображение (28x28 пикселей) в вектор (784 пикселей).\n",
    "\n",
    "[**Linear**](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) -  применяет линейную трансформацию к входным данным используя веса модели и смещение (bias):\n",
    "<img src=\"images/linear.png\">\n",
    "\n",
    "[**ReLU**](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) -  функция активации, добавляет нейлинейность в модель, что позволяет модели аппроксимировать нейлинейные функции:\n",
    "<img src=\"images/relu.png\">\n",
    "\n",
    "[**Sequential**](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) -  упорядоченный контейнер модулей нейронной сети, предназначенный для быстрого создания сетей с последовательной архитектурой. Данные будут проходить слои модели в том порядке, в котором они описаны. \n",
    "\n",
    "[**Softmax**](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) -  это обобщение логистической функции на несколько измерений, используется для нормализации выходных данных сети (в диапазон значений [0,1], которые в сумме дают 1):\n",
    "<img src=\"images/softmax.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выбираем девайс (cpu или gpu), на котором будут происходит вычисления:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Определяем кастомную модель (Sequential):\n",
    "class NeuralNetworkSeq(nn.Module):\n",
    "    def __init__(self, input_size=(28,28,1), output_size=10):\n",
    "        super(NeuralNetworkSeq, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.output = nn.Linear(512, output_size)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size[0]*input_size[1]*input_size[2], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            self.output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model_seq = NeuralNetworkSeq().to(device)\n",
    "print(model_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228db4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем кастомную модель:\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size=(28,28,1), output_size=10):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size[0]*input_size[1]*input_size[2], 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.output = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        logits = self.output(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfa234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мы можем итерироваться через параметры (веса) нашей модели:\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Имя: {name} | Размер: {param.size()} | Значения : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пропустим рандомное изображение из MNIST через нашу еще необученную модель:\n",
    "sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "X, Y = training_data[sample_idx]\n",
    "X = X.to(device)\n",
    "\n",
    "# Forward pass:\n",
    "logits = model(X)\n",
    "pred_prob = nn.Softmax(dim=1)(logits)\n",
    "Y_pred = pred_prob.argmax(1)\n",
    "\n",
    "print('Ненормализованные вероятности:\\n', logits.cpu().detach())\n",
    "print('Нормализованные вероятности:\\n', pred_prob.cpu().detach())\n",
    "text = f\"Prediction: {Y_pred.item()} / Ground Truth: {Y}\"\n",
    "show(X.cpu().squeeze(), title=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d949c6",
   "metadata": {},
   "source": [
    "## 3 Обучение (оптимизация параметров модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03192a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обучения модели:\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Делаем предсказания:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            \n",
    "# Функция для тестирования обученной модели:\n",
    "def test(dataloader, model, loss_fn, verbose=True, iterations=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            if iterations is not None and i >= iterations:\n",
    "                break\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if verbose:\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf780f05",
   "metadata": {},
   "source": [
    "### Функции потерь / Loss Functions\n",
    "\n",
    "[**MSELoss**](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) -  Mean Square Error / Squared L2 (среднеквадратичная ошибка / квадратичная L2 норма:\n",
    "<img src=\"images/mse.png\">\n",
    "\n",
    "[**CrossEntropyLoss**](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) -  Перекрестная энтропия, комбинирует Softmax и Negative Log Likelihood:\n",
    "<img src=\"images/crossentropy.png\">\n",
    "\n",
    "### Оптимайзеры / Optimizers\n",
    "[**SGD**](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html?highlight=sgd#torch.optim.SGD) -  Стохастический градиентный спуск, принимает аргументы:\n",
    "- параметры модели, \n",
    "- learning rate (скорость обучения),\n",
    "- momentum (импульс), \n",
    "- weight_decay (распад весов, помогает регуляризировать модель, тем самым предотвращая переобучения).\n",
    "\n",
    "[**Adam**](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html?highlight=adam#torch.optim.Adam) -  Оптимизационный алгоритм Адама:\n",
    "- параметры модели, \n",
    "- learning rate (скорость обучения),\n",
    "- betas, \n",
    "- weight_decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de6473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Инициализируем модель и data loaders:\n",
    "model = NeuralNetwork().to(device)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Задаем гипперпараметры:\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "\n",
    "# Выбираем функцию потерь и оптимайзер:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# Обучаем 5 эпох (эпоха - один проход по всем данным):\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d379249",
   "metadata": {},
   "source": [
    "## 4 Сохранение и загрузка обученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03aa4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем checkpoint:\n",
    "checkpoint_path = './checkpoints/mnist_checkpoint.pth'\n",
    "torch.save(model.state_dict(), checkpoint_path)\n",
    "print(\"Saved PyTorch Model State to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем модель (объект класса NeuralNetwork) и загружаем параметры из checkpoint:\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "print (model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95548961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируем загруженную модель (чтобы убедиться что все правильно загрузилось):\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc2a0c",
   "metadata": {},
   "source": [
    "## 5 Делаем предсказания (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Опишем название классов:\n",
    "classes = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "sample_idx = torch.randint(len(test_data), size=(1,)).item()\n",
    "x, y = test_data[sample_idx][0], test_data[sample_idx][1]\n",
    "x = x.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    text = f\"Prediction: {predicted} / Ground Truth: {actual}\"\n",
    "    show(x.cpu().squeeze(), title=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083731c",
   "metadata": {},
   "source": [
    "## 6 Обучаем сверточную сеть\n",
    "LeNet архитектура:\n",
    "<img src=\"images/cnn.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fa7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F     # imports activation functions\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(CNN, self).__init__()\n",
    "        # Describe layers:\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)     # 1: in channels, 32: out channels, 3: kernel size, 1: stride\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)         # 9216: in channels, 128: out channels\n",
    "        self.fc2 = nn.Linear(128, out_size)\n",
    "\n",
    "    # Forward propagation:\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x reprenets our input data\n",
    "        '''\n",
    "        # Pass data through conv's layers:\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x:\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Flatten x with start_dim=1\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        # Pass data through FC's layers:\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "cnn_model = CNN(10).to(device)\n",
    "print(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b41fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Инициализируем модель и data loaders:\n",
    "cnn_model = CNN(10).to(device)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Задаем гипперпараметры:\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "\n",
    "# Выбираем функцию потерь и оптимайзер:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(cnn_model.parameters(), lr=lr, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# Обучаем 5 эпох:\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, cnn_model, loss_fn, optimizer)\n",
    "    test(test_dataloader, cnn_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dcdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируем обученную модель:\n",
    "cnn_model = cnn_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test(test_dataloader, cnn_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983391cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Опишем название классов:\n",
    "classes = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "# Делаем предсказание:\n",
    "sample_idx = 444\n",
    "x, y = test_data[sample_idx][0], test_data[sample_idx][1]\n",
    "x = x.to(device)\n",
    "\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = cnn_model(x.unsqueeze(0))\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    text = f\"Prediction: {predicted} / Ground Truth: {actual}\"\n",
    "    show(x.cpu().squeeze(), title=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1296ab9",
   "metadata": {},
   "source": [
    "# Ускорерние инференеса в PyTorch 2.0\n",
    "\n",
    "`torch.compile(model=None, *, fullgraph=False, dynamic=False, backend='inductor', mode=None, options=None, disable=False)`\n",
    "\n",
    "Optimizes given model/function using TorchDynamo and specified backend.\n",
    "\n",
    "Parameters:\n",
    "- model (Callable) – Module/function to optimize\n",
    "- fullgraph (bool) – Whether it is ok to break model into several subgraphs\n",
    "- dynamic (bool) – Use dynamic shape tracing\n",
    "- backend (str or Callable) – backend to be used\n",
    "- mode (str) – Can be either “default”, “reduce-overhead” or “max-autotune”\n",
    "- options (dict) – A dictionary of options to pass to the backend.\n",
    "- disable (bool) – Turn torch.compile() into a no-op for testing\n",
    "\n",
    "## Компиляция кастомной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082d44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Модель и лосс функция:\n",
    "cnn_model = cnn_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Замерим время инференса модели без компиляции:\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    test(test_dataloader, cnn_model, loss_fn, verbose=False)\n",
    "print('W/O compile:', time.time() - start)\n",
    "\n",
    "\n",
    "# Warm-up для компиляции:\n",
    "compiled_cnn_model_ro = torch.compile(cnn_model, mode=\"reduce-overhead\")\n",
    "test(test_dataloader, compiled_cnn_model_ro, loss_fn, verbose=False)\n",
    "\n",
    "\n",
    "# Компилируем с defualt параметрами: \n",
    "# - быстрое время компиляции\n",
    "# - не использует дополнительную память для компиляции\n",
    "compiled_cnn_model = torch.compile(cnn_model)\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    test(test_dataloader, compiled_cnn_model, loss_fn, verbose=False)\n",
    "print('default:', time.time() - start)\n",
    "\n",
    "\n",
    "# Компиляция reduce-overhead:\n",
    "# - хорошо ускоряет небольшие модели\n",
    "# - снижает накладные расходы фреймворка\n",
    "# - использует дополнительную память для компиляци\n",
    "compiled_cnn_model_ro = torch.compile(cnn_model, mode=\"reduce-overhead\")\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    test(test_dataloader, compiled_cnn_model_ro, loss_fn, verbose=False)\n",
    "print('reduce-overhead:', time.time() - start)\n",
    "\n",
    "\n",
    "# Компиляция max-autotune: \n",
    "# - выдает максимально быстрый код,\n",
    "# - занимает значительное время для компиляции\n",
    "compiled_cnn_model_ma = torch.compile(cnn_model, mode=\"max-autotune\")\n",
    "start = time.time()\n",
    "for i in range(10):\n",
    "    test(test_dataloader, compiled_cnn_model_ma, loss_fn, verbose=False)\n",
    "print('max-autotune:', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc8a8b",
   "metadata": {},
   "source": [
    "## Компиляция моделей torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20717fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size=32\n",
    "\n",
    "\n",
    "# Скачиваем CIFAR датасет и приводим его к размеру 224x224 для тестирования torchvision моделей:\n",
    "test_data_cifar10_224 = datasets.CIFAR10(\n",
    "    root=\"./dataset_cifar\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    ")\n",
    "\n",
    "test_dataloader_cifar10_224 = DataLoader(test_data_cifar10_224, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Функция для сравнения скомплириованных и оригинальных моделей:\n",
    "#@torch._dynamo.optimize('inductor')\n",
    "def compare(model, iters=1, test_batches=None):\n",
    "    '''\n",
    "    iters:        число запусков теста\n",
    "    test_batches: число батчей в тесте\n",
    "    '''\n",
    "    # Без компиляции:\n",
    "    start = time.time()\n",
    "    for i in range(iters):\n",
    "        test(test_dataloader_cifar10_224, vision_model, nn.CrossEntropyLoss(), False, test_batches)\n",
    "    wo_elapsed = time.time() - start\n",
    "    print('W/O compile:', wo_elapsed)\n",
    "    \n",
    "    \n",
    "    # Warm-up compilation:\n",
    "#     torch._dynamo.reset()\n",
    "    compiled_vision_model = torch.compile(vision_model, mode=\"reduce-overhead\")\n",
    "    test(test_dataloader_cifar10_224, compiled_vision_model, nn.CrossEntropyLoss(), False, test_batches)\n",
    "\n",
    "\n",
    "    # С компиляцией default:\n",
    "    compiled_vision_model = torch.compile(vision_model)\n",
    "    start = time.time()\n",
    "    for i in range(iters):\n",
    "        test(test_dataloader_cifar10_224, compiled_vision_model, nn.CrossEntropyLoss(), False, test_batches)\n",
    "    elapsed = time.time() - start\n",
    "    print(f'default: {elapsed}, speedup {wo_elapsed/elapsed}')\n",
    "\n",
    "\n",
    "    # С компиляцией reduce-overhead:\n",
    "    compiled_vision_model = torch.compile(vision_model, mode=\"reduce-overhead\")\n",
    "    start = time.time()\n",
    "    for i in range(iters):\n",
    "        test(test_dataloader_cifar10_224, compiled_vision_model, nn.CrossEntropyLoss(), False, test_batches)\n",
    "    elapsed = time.time() - start\n",
    "    print(f'reduce-overhead: {elapsed}, speedup {wo_elapsed/elapsed}')\n",
    "\n",
    "\n",
    "    # С компиляцией max-autotune:\n",
    "    compiled_vision_model = torch.compile(vision_model, mode=\"max-autotune\")\n",
    "    start = time.time()\n",
    "    for i in range(iters):\n",
    "        test(test_dataloader_cifar10_224, compiled_vision_model, nn.CrossEntropyLoss(), False, test_batches)\n",
    "    elapsed = time.time() - start\n",
    "    print(f'max-autotune: {elapsed}, speedup {wo_elapsed/elapsed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8598cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters, test_batches = 3, 10\n",
    "\n",
    "# Выбираем torchvision модель:\n",
    "vision_model = torchvision.models.resnet50().to(device)\n",
    "print('\\n### resnet50 ###')\n",
    "compare(vision_model, iters, test_batches)\n",
    "\n",
    "\n",
    "vision_model = torchvision.models.mobilenet_v3_small().to(device)\n",
    "print('\\n### mobilenet_v3_small ###')\n",
    "compare(vision_model, iters, test_batches)\n",
    "\n",
    "\n",
    "vision_model = torchvision.models.mobilenet_v3_large().to(device)\n",
    "print('\\n### mobilenet_v3_large ###')\n",
    "compare(vision_model, iters, test_batches)\n",
    "\n",
    "\n",
    "vision_model = torchvision.models.efficientnet_v2_s().to(device)\n",
    "print('\\n### efficientnet_v2_s ###')\n",
    "compare(vision_model, iters, test_batches)\n",
    "\n",
    "\n",
    "vision_model = torchvision.models.efficientnet_v2_s().to(device)\n",
    "print('\\n### efficientnet_v2_l ###')\n",
    "compare(vision_model, iters, test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03e50b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from torch import dynamo\n",
    "#print (torch._dynamo.list_backends())\n",
    "\n",
    "# Swin трансформер:\n",
    "vision_model = torchvision.models.swin_s().to(device)\n",
    "print('\\n### swin_s ###')\n",
    "compare(vision_model, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a103091",
   "metadata": {},
   "source": [
    "## 7 Обучаем полносвязную сеть на CIFAR10 (rgb, 32x32, 10 классов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e412d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачиваем данные для обучения:\n",
    "training_data_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./dataset_cifar\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# Скачиваем данные для теста:\n",
    "test_data_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./dataset_cifar\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962866cb",
   "metadata": {},
   "source": [
    "**Визуализация данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Опишем название классов:\n",
    "labels_map = ['airplanes','cars','birds','cats','deer','dogs','frogs','horses','ships','trucks']\n",
    " \n",
    "# Визуализируем рандомные сэмплы из MNIST:\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data_cifar10), size=(1,)).item()\n",
    "    img, label = training_data_cifar10[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.permute(1, 2, 0))    # переставляем каналы [C,H,W] -> [H,W,C]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7d918",
   "metadata": {},
   "source": [
    "**Обучаем модель на CIFAR10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Инициализируем модель и data loaders:\n",
    "model = NeuralNetwork(input_size=(32,32,3)).to(device)\n",
    "train_dataloader_cifar10 = DataLoader(training_data_cifar10, batch_size=batch_size)\n",
    "test_dataloader_cifar10 = DataLoader(test_data_cifar10, batch_size=batch_size)\n",
    "\n",
    "# Задаем гипперпараметры:\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "\n",
    "# Выбираем функцию потерь и оптимайзер:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# Обучаем 5 эпох (эпоха - один проход по всем данным):\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader_cifar10, model, loss_fn, optimizer)\n",
    "    test(test_dataloader_cifar10, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e567093b",
   "metadata": {},
   "source": [
    "## 8 Обучаем полносвязную сеть на CIFAR10 (grayscale, 28x28, 10 классов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скачиваем данные для обучения:\n",
    "training_data_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./dataset_cifar\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "# Скачиваем данные для теста:\n",
    "test_data_cifar10 = datasets.CIFAR10(\n",
    "    root=\"./dataset_cifar\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1), \n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Опишем название классов:\n",
    "labels_map = ['airplanes','cars','birds','cats','deer','dogs','frogs','horses','ships','trucks']\n",
    " \n",
    "# Визуализируем рандомные сэмплы из MNIST:\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data_cifar10), size=(1,)).item()\n",
    "    img, label = training_data_cifar10[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\") # squeeze() - удаляем все измерения размером 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0274e21b",
   "metadata": {},
   "source": [
    "**Обучаем модель на CIFAR10 (grayscale, 28x28)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем модель и data loaders:\n",
    "model = NeuralNetwork(input_size=(28,28,1)).to(device)\n",
    "train_dataloader_cifar10 = DataLoader(training_data_cifar10, batch_size=batch_size)\n",
    "test_dataloader_cifar10 = DataLoader(test_data_cifar10, batch_size=batch_size)\n",
    "\n",
    "# Задаем гипперпараметры:\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "\n",
    "# Выбираем функцию потерь и оптимайзер:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# Обучаем 5 эпох (эпоха - один проход по всем данным):\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader_cifar10, model, loss_fn, optimizer)\n",
    "    test(test_dataloader_cifar10, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4d58d",
   "metadata": {},
   "source": [
    "## 9 Transfer Learning MNIST -> CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем веса MNIST из checkpoint:\n",
    "checkpoint_path = './checkpoints/mnist_checkpoint.pth'\n",
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "# Протестируем загруженную модель на MNIST:\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f4993e",
   "metadata": {},
   "source": [
    "**Заморозим веса всех слоев кроме последнего**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отключаем градиент (кроме output слоя):\n",
    "for param in model.parameters(): # Проходим по параметрам модели (каждый параметр - это каждый слой)\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Пересоздаем выходной слой на 10 нейронов:\n",
    "mnist_output = model.output                       # Сохраняем выходной слой MNIST\n",
    "in_features = model.output.in_features\n",
    "model.output = torch.nn.Linear(in_features, 10, bias=True)\n",
    "model.output = model.output.cuda()                # Перенесем на cuda\n",
    "\n",
    "# Проверяем:\n",
    "for name, param in model.named_parameters():\n",
    "    print (name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba27820",
   "metadata": {},
   "source": [
    "**Протестируем модель, обученную на MNIST, на CIFAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем data loaders:\n",
    "train_dataloader_cifar10 = DataLoader(training_data_cifar10, batch_size=batch_size)\n",
    "test_dataloader_cifar10 = DataLoader(test_data_cifar10, batch_size=batch_size)\n",
    "\n",
    "# Протестируем модель, обученную на MNIST, на CIFAR:\n",
    "test(test_dataloader_cifar10, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c02ea7",
   "metadata": {},
   "source": [
    "**Дообучим последний слой модели на CIFAR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539db513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем гипперпараметры:\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "\n",
    "# Выбираем функцию потерь и оптимайзер:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "\n",
    "# Обучаем 5 эпох (эпоха - один проход по всем данным):\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader_cifar10, model, loss_fn, optimizer)\n",
    "    test(test_dataloader_cifar10, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab04a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Восстановим выходной слой MNIST:\n",
    "model.output = mnist_output\n",
    "\n",
    "# Протестируем модель еще раз на MNIST:\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4786e32",
   "metadata": {},
   "source": [
    "## Теперь вы знаете как создать и обучить свою нейронную сеть!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем checkpoint:\n",
    "checkpoint_path = 'Checkpoints/cnn_mnist_checkpoint.pth'\n",
    "torch.save(cnn_model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8defa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем checkpoint:\n",
    "checkpoint_path = 'Checkpoints/cnn_mnist_checkpoint.pth'\n",
    "cnn_model = CNN(10)\n",
    "cnn_model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "# Сохраняем модель как TorchScript:\n",
    "cnn_model.eval()\n",
    "example_input = torch.rand(1, 1, 28, 28) \n",
    "traced_cnn_model = torch.jit.trace(cnn_model, example_input)\n",
    "out = traced_cnn_model(example_input)\n",
    "\n",
    "# Convert to Core ML using the Unified Conversion API:\n",
    "coreml_model = ct.convert(\n",
    "    traced_cnn_model,\n",
    "    inputs=[ct.TensorType(shape=example_input.shape)]\n",
    " )\n",
    "\n",
    "# Save the converted model:\n",
    "coreml_model.save(\"Checkpoints/cnn_mnist.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import _dynamo\n",
    "\n",
    "\n",
    "print (torch._dynamo.list_backends())\n",
    "\n",
    "@torch._dynamo.optimize('inductor')\n",
    "def fun(x):\n",
    "    x = torch.add(x, 1)\n",
    "    x = torch.sub(x, 1)\n",
    "    x = torch.add(x, 1)\n",
    "    return x\n",
    "\n",
    "tensor = torch.tensor((500,500,700))\n",
    "fun(tensor)\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    fun(tensor)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff21e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
